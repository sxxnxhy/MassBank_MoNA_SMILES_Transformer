import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import torch
import numpy as np
from tqdm import tqdm
from collections import defaultdict
import math
import torch.nn.functional as F
from torch.amp import autocast

import config
from dataset import prepare_dataloaders
from model import CLIPModel

# --- [ì„¤ì •] ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CHECKPOINT_PATH = f"{config.CHECKPOINT_DIR}/best_model.pt"

# Few-Shot ì„¤ì •
N_WAY = 5
N_QUERY = 5
N_EPISODES = 600

def load_model_and_data():
    """ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë”ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
    print("\n[1/4] Loading Data & Model...")
    
    # 1. ë°ì´í„° ë¡œë“œ (Test Loaderë§Œ ì‚¬ìš©)
    _, val_loader = prepare_dataloaders()
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    model = CLIPModel().to(DEVICE)
    
    # 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if os.path.exists(CHECKPOINT_PATH):
        print(f"Loading Checkpoint: {CHECKPOINT_PATH}")
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… Loaded model from Epoch {checkpoint['epoch']}")
    else:
        raise FileNotFoundError(f"Checkpoint not found at {CHECKPOINT_PATH}")
        
    return model, val_loader

@torch.no_grad()
def encode_all_data(model, val_loader):
    """
    í…ŒìŠ¤íŠ¸ ì…‹ ì „ì²´ë¥¼ í•œ ë²ˆë§Œ ì¸ì½”ë”©í•˜ì—¬ GPU/CPU ë©”ëª¨ë¦¬ì— ì €ìž¥í•©ë‹ˆë‹¤.
    ëª¨ë“  ì‹¤í—˜ì—ì„œ ì´ ìž„ë² ë”©ì„ ìž¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    model.eval()
    print("\n[2/4] Encoding All Test Data (Running Inference Once)...")
    
    all_spec_embeds = []
    all_text_embeds = []
    all_input_ids = [] # í´ëž˜ìŠ¤ ì‹ë³„ìš© (SMILES ëŒ€ìš©)
    
    for batch in tqdm(val_loader, desc="Encoding"):
        peak_sequence = batch['peak_sequence'].to(DEVICE).float()
        peak_mask = batch['peak_mask'].to(DEVICE)
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        
        with autocast(device_type='cuda', dtype=torch.float16):
            spec_emb = model.ms_encoder(peak_sequence, peak_mask)
            text_emb = model.text_encoder(input_ids, attention_mask)
        
        # CPUë¡œ ì´ë™í•˜ì—¬ ì €ìž¥
        all_spec_embeds.append(spec_emb.cpu())
        all_text_embeds.append(text_emb.cpu())
        all_input_ids.append(input_ids.cpu())
        
    # í…ì„œ í•©ì¹˜ê¸°
    all_spec_embeds = torch.cat(all_spec_embeds, dim=0).float()
    all_text_embeds = torch.cat(all_text_embeds, dim=0).float()
    all_input_ids = torch.cat(all_input_ids, dim=0)
    
    print(f"âœ… Encoded {all_spec_embeds.shape[0]} samples.")
    return all_spec_embeds, all_text_embeds, all_input_ids

# ---------------------------------------------------------
# ì‹¤í—˜ 1 & 2: Zero-Shot Retrieval (Global vs Benchmark)
# ---------------------------------------------------------
def evaluate_retrieval(spec_embeds, text_embeds, k_candidates=None):
    """
    k_candidates=None -> Global (Hard)
    k_candidates=256 -> Benchmark (Standard)
    """
    num_samples = spec_embeds.shape[0]
    mode_name = f"Pool: {k_candidates}" if k_candidates else f"Pool: ALL ({num_samples})"
    print(f"\n   Running Retrieval Eval [{mode_name}]...")
    
    # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    sim_matrix = spec_embeds.to(DEVICE) @ text_embeds.to(DEVICE).T
    
    hits_1, hits_5, hits_10 = 0, 0, 0
    
    # --- Case A: Global Retrieval (ì „ì²´ ê²€ìƒ‰) ---
    if k_candidates is None or k_candidates >= num_samples:
        ground_truth = torch.arange(num_samples, device=DEVICE, dtype=torch.long)
        _, top10_indices = torch.topk(sim_matrix, k=10, dim=1)
        
        hits_1 = (top10_indices[:, :1] == ground_truth.view(-1, 1)).any(dim=1).sum().item()
        hits_5 = (top10_indices[:, :5] == ground_truth.view(-1, 1)).any(dim=1).sum().item()
        hits_10 = (top10_indices[:, :10] == ground_truth.view(-1, 1)).any(dim=1).sum().item()
        
    # --- Case B: Benchmark Subsampling (256ê°œ ì¤‘ ê²€ìƒ‰) ---
    else:
        torch.manual_seed(config.RANDOM_SEED) # ìž¬í˜„ì„±
        
        for i in range(num_samples):
            correct_score = sim_matrix[i, i]
            
            # ë‚˜ ìžì‹  ì œì™¸í•œ ì˜¤ë‹µ ì ìˆ˜ë“¤
            neg_indices = torch.arange(num_samples) != i
            negative_scores = sim_matrix[i, neg_indices]
            
            # 255ê°œ ëžœë¤ ìƒ˜í”Œë§
            n_neg = k_candidates - 1
            perm = torch.randperm(len(negative_scores))[:n_neg]
            sampled_negatives = negative_scores[perm]
            
            # ëž­í‚¹ (ì ìˆ˜ê°€ ë” ë†’ì€ ì˜¤ë‹µ ê°œìˆ˜ + 1)
            # ë™ì ìž ì²˜ë¦¬ë¥¼ ìœ„í•´ > ì‚¬ìš© (ì¼ë°˜ì  ê¸°ì¤€)
            rank = (sampled_negatives > correct_score).sum().item() + 1
            
            if rank == 1: hits_1 += 1
            if rank <= 5: hits_5 += 1
            if rank <= 10: hits_10 += 1
            
    return {
        'R@1': hits_1 / num_samples * 100,
        'R@5': hits_5 / num_samples * 100,
        'R@10': hits_10 / num_samples * 100
    }

# ---------------------------------------------------------
# ì‹¤í—˜ 3 & 4: Few-Shot Classification (1-Shot & 5-Shot)
# ---------------------------------------------------------
def evaluate_few_shot(spec_embeds, input_ids, n_way=5, k_shot=1, n_query=5, n_episodes=600):
    print(f"\n   Running {k_shot}-Shot {n_way}-Way Classification...")
    
    # 1. í´ëž˜ìŠ¤ë³„ ì¸ë±ì‹±
    class_indices = defaultdict(list)
    for idx, token_ids in enumerate(input_ids):
        # Tensor -> Tuple (Hashable Key)
        key = tuple(token_ids.tolist())
        class_indices[key].append(idx)
        
    # ìœ íš¨ í´ëž˜ìŠ¤ í•„í„°ë§ (ë°ì´í„° ì¶©ë¶„í•œ ê²ƒë§Œ)
    min_samples = k_shot + n_query
    valid_classes = [k for k, v in class_indices.items() if len(v) >= min_samples]
    
    if len(valid_classes) < n_way:
        print(f"âš ï¸ Error: Not enough classes with {min_samples} samples.")
        return 0.0, 0.0

    accuracies = []
    
    # 2. ì—í”¼ì†Œë“œ ë°˜ë³µ
    for _ in range(n_episodes):
        # í´ëž˜ìŠ¤ ëžœë¤ ì„ íƒ
        chosen_keys_idx = np.random.choice(len(valid_classes), n_way, replace=False)
        chosen_keys = [valid_classes[i] for i in chosen_keys_idx]
        
        support_set = []
        query_set = []
        query_labels = []
        
        for label_idx, key in enumerate(chosen_keys):
            indices = class_indices[key]
            selected_indices = np.random.choice(indices, k_shot + n_query, replace=False)
            
            # Support & Query ë¶„ë¦¬
            sup_idx = selected_indices[:k_shot]
            qry_idx = selected_indices[k_shot:]
            
            # ìž„ë² ë”© ê°€ì ¸ì˜¤ê¸° (GPUë¡œ ì´ë™)
            # k_shotì´ 1ì¼ ë•Œë„ ì°¨ì› ìœ ì§€ë¥¼ ìœ„í•´ stack ì‚¬ìš©
            sup_emb = torch.stack([spec_embeds[i] for i in sup_idx]).to(DEVICE)
            qry_emb = torch.stack([spec_embeds[i] for i in qry_idx]).to(DEVICE)
            
            # í”„ë¡œí† íƒ€ìž… (í‰ê· )
            prototype = sup_emb.mean(dim=0)
            support_set.append(prototype)
            
            query_set.append(qry_emb)
            query_labels.extend([label_idx] * n_query)
            
        # í…ì„œ ë³€í™˜ [N_way, Dim], [Total_Query, Dim]
        prototypes = torch.stack(support_set)
        queries = torch.cat(query_set)
        labels = torch.tensor(query_labels).to(DEVICE)
        
        # ê±°ë¦¬ ê³„ì‚° & ì˜ˆì¸¡
        logits = torch.matmul(queries, prototypes.T)
        preds = logits.argmax(dim=1)
        
        acc = (preds == labels).float().mean().item()
        accuracies.append(acc)
        
    mean = np.mean(accuracies) * 100
    std = np.std(accuracies) * 100
    return mean, std

# ---------------------------------------------------------
# Main Execution
# ---------------------------------------------------------
def main():
    # 1. ëª¨ë¸ & ë°ì´í„° ë¡œë“œ
    model, val_loader = load_model_and_data()
    
    # 2. ì „ì²´ ë°ì´í„° ì¸ì½”ë”© (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    spec_embeds, text_embeds, input_ids = encode_all_data(model, val_loader)
    
    print("\n" + "="*60)
    print("ðŸš€ FINAL RESULTS SUMMARY")
    print("="*60)
    
    # --- [Result 1] Zero-Shot (Hard Mode: 1 vs All) ---
    res1 = evaluate_retrieval(spec_embeds, text_embeds, k_candidates=None)
    print(f"1. Zero-Shot Global Retrieval (Hard Mode, vs {spec_embeds.shape[0]})")
    print(f"   R@1: {res1['R@1']:.2f}% | R@10: {res1['R@10']:.2f}%")
    print("-" * 60)
    
    # --- [Result 2] Zero-Shot (Benchmark: 1 vs 256) ---
    res2 = evaluate_retrieval(spec_embeds, text_embeds, k_candidates=256)
    print(f"2. Zero-Shot Benchmark Retrieval (Standard, vs 256)")
    print(f"   R@1: {res2['R@1']:.2f}% | R@10: {res2['R@10']:.2f}%")
    print("-" * 60)
    
    # --- [Result 3] 1-Shot Classification ---
    acc_1shot, std_1shot = evaluate_few_shot(
        spec_embeds, input_ids, n_way=N_WAY, k_shot=1, n_query=N_QUERY, n_episodes=N_EPISODES
    )
    print(f"3. Few-Shot Classification (1-Shot, 5-Way)")
    print(f"   Accuracy: {acc_1shot:.2f}% Â± {std_1shot:.2f}%")
    print("-" * 60)

    # --- [Result 4] 5-Shot Classification ---
    acc_5shot, std_5shot = evaluate_few_shot(
        spec_embeds, input_ids, n_way=N_WAY, k_shot=5, n_query=N_QUERY, n_episodes=N_EPISODES
    )
    print(f"4. Few-Shot Classification (5-Shot, 5-Way)")
    print(f"   Accuracy: {acc_5shot:.2f}% Â± {std_5shot:.2f}%")
    print("=" * 60)

if __name__ == '__main__':
    main()