import torch
import numpy as np
from tqdm import tqdm
from collections import defaultdict
import config
from dataset import prepare_dataloaders
from model import CLIPModel
import torch.nn.functional as F

# --- [ì„¤ì •] ----------------------
K_SHOT_LIST = [1, 3, 5, 10]  # í…ŒìŠ¤íŠ¸í•  ìƒ· ìˆ˜ (1ì¥, 3ì¥, 5ì¥, 10ì¥ ì¤¬ì„ ë•Œ ì„±ëŠ¥ ë³€í™”)
# ---------------------------------

@torch.no_grad()
def evaluate_few_shot_retrieval_scan(model, dataloader, device, k_shots=[1, 5]):
    model.eval()
    print(f"\n" + "="*60)
    print(f"ğŸ” Running Few-Shot Retrieval Benchmark (K={k_shots})")
    print("Logic: Average K spectra -> Retrieve correct SMILES from FULL database")
    print("="*60)

    # 1. ë°ì´í„° ì¸ì½”ë”© ë° ê·¸ë£¹í•‘
    print("Encoding test set and grouping by SMILES...")
    
    # Key: SMILES string, Value: List of Spectrum Embeddings
    mol_to_specs = defaultdict(list)
    # Key: SMILES string, Value: Text Embedding (1ê°œë§Œ ìˆìœ¼ë©´ ë¨)
    mol_to_text_emb = {}
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for batch in tqdm(dataloader, desc="Encoding"):
        peak_sequence = batch['peak_sequence'].to(device).float()
        peak_mask = batch['peak_mask'].to(device)
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        
        # í…ìŠ¤íŠ¸ ë””ì½”ë”© (ê·¸ë£¹í•‘ í‚¤ë¡œ ì‚¬ìš©)
        # ì£¼ì˜: ì‹¤ì œ dataset.pyì— get_tokenizerê°€ ìˆì–´ì•¼ í•¨. ì—†ìœ¼ë©´ configì—ì„œ ë¡œë“œ.
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(config.TEXT_ENCODER['model_name'])
        smiles_list = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
        
        # ì„ë² ë”© ì¶”ì¶œ
        with torch.cuda.amp.autocast():
            spec_emb = model.ms_encoder(peak_sequence, peak_mask)
            text_emb = model.text_encoder(input_ids, attention_mask)
            
        spec_emb = spec_emb.cpu()
        text_emb = text_emb.cpu()
        
        for i, smile in enumerate(smiles_list):
            # ê³µë°± ì œê±° (í† í¬ë‚˜ì´ì € ë””ì½”ë”© ì‹œ ìƒê¸¸ ìˆ˜ ìˆëŠ” ê³µë°± ì²˜ë¦¬)
            smile_key = smile.replace(" ", "") 
            
            mol_to_specs[smile_key].append(spec_emb[i])
            if smile_key not in mol_to_text_emb:
                mol_to_text_emb[smile_key] = text_emb[i]

    # 2. ê²€ìƒ‰ ëŒ€ìƒ(Candidate Pool) êµ¬ì¶•
    # ì „ì²´ ìœ ë‹ˆí¬í•œ SMILESë“¤ì˜ í…ìŠ¤íŠ¸ ì„ë² ë”© í–‰ë ¬
    unique_smiles = list(mol_to_text_emb.keys())
    candidate_embeddings = torch.stack([mol_to_text_emb[s] for s in unique_smiles]) # [N_unique, Dim]
    candidate_embeddings = F.normalize(candidate_embeddings, p=2, dim=1) # ì •ê·œí™”
    
    print(f"\nCandidate Pool Size (Unique Molecules): {len(unique_smiles)}")
    
    # 3. K-Shot ë³„ ì„±ëŠ¥ ì¸¡ì •
    for k in k_shots:
        print(f"\n--- Testing {k}-Shot Retrieval ---")
        
        r1_hits = 0
        r5_hits = 0
        r10_hits = 0
        total_queries = 0
        
        # ê° ë¶„ìë§ˆë‹¤ ë£¨í”„
        for target_smile in tqdm(unique_smiles, desc=f"Retrieving (K={k})"):
            specs = mol_to_specs[target_smile]
            
            # ìŠ¤í™íŠ¸ëŸ¼ ê°œìˆ˜ê°€ Kê°œ ë¯¸ë§Œì´ë©´ í…ŒìŠ¤íŠ¸ ë¶ˆê°€ (ìŠ¤í‚µ)
            if len(specs) < k:
                continue
                
            # Kê°œ ëœë¤ ìƒ˜í”Œë§ (ë¹„ë³µì›) -> í‰ê·  ë²¡í„° ìƒì„±
            # ì‹¤í—˜ì˜ ì•ˆì •ì„±ì„ ìœ„í•´, ê°€ëŠ¥í•œ ê²½ìš° ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§í•´ì„œ í‰ê· ë‚¼ ìˆ˜ë„ ìˆì§€ë§Œ
            # ì—¬ê¸°ì„œëŠ” 1ë²ˆë§Œ ìˆ˜í–‰ (Standard Protocol)
            indices = np.random.choice(len(specs), k, replace=False)
            selected_specs = torch.stack([specs[i] for i in indices]) # [K, Dim]
            
            # [í•µì‹¬] Mean Pooling (ë²¡í„° í‰ê· )
            query_vec = torch.mean(selected_specs, dim=0, keepdim=True) # [1, Dim]
            query_vec = F.normalize(query_vec, p=2, dim=1)
            
            # ìœ ì‚¬ë„ ê³„ì‚° (1 vs N)
            sim_scores = torch.matmul(query_vec, candidate_embeddings.T).squeeze() # [N_unique]
            
            # ë­í‚¹ ê³„ì‚°
            # ì •ë‹µ ì¸ë±ìŠ¤ ì°¾ê¸°
            target_idx = unique_smiles.index(target_smile)
            
            # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ì •ë‹µ ë“±ìˆ˜ í™•ì¸
            # (argsortëŠ” ì˜¤ë¦„ì°¨ìˆœì´ë¯€ë¡œ ë’¤ì§‘ê±°ë‚˜, 'ë³´ë‹¤ í° ê°’ì˜ ê°œìˆ˜'ë¥¼ ì…ˆ)
            score_target = sim_scores[target_idx]
            rank = (sim_scores > score_target).sum().item() + 1
            
            if rank == 1: r1_hits += 1
            if rank <= 5: r5_hits += 1
            if rank <= 10: r10_hits += 1
            total_queries += 1
            
        # ê²°ê³¼ ì¶œë ¥
        if total_queries == 0:
            print("  Warning: No molecules had enough spectra for this K.")
        else:
            print(f"  Samples evaluated: {total_queries}")
            print(f"  R@1 : {r1_hits/total_queries*100:.2f}%")
            print(f"  R@5 : {r5_hits/total_queries*100:.2f}%")
            print(f"  R@10: {r10_hits/total_queries*100:.2f}%")

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
    print(f"Using Device: {device}")
    
    # ë°ì´í„° ë¡œë“œ
    _, test_loader = prepare_dataloaders()
    
    # ëª¨ë¸ ë¡œë“œ
    model = CLIPModel().to(device)
    checkpoint = torch.load(f"{config.CHECKPOINT_DIR}/best_model.pt", map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded model (Epoch {checkpoint['epoch']})")
    
    # í‰ê°€ ì‹¤í–‰
    evaluate_few_shot_retrieval_scan(model, test_loader, device, k_shots=K_SHOT_LIST)

if __name__ == '__main__':
    main()